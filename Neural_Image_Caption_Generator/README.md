# Show and tell - Neural Image Caption Generator
This project is to reimplement the show and tell paper (Vinyals et al., 2015). This application helps visually-impaired people by transforming visual signals into proper language, which involves tasks of both image classification as well as natural language processing. Using 500 GPU hours for training, the trainning process managed to improve the performance and the convergence by replacing the pre-trained model to ResNet 152, using Adam optimizer and several other experiments. As a result, the model yields better performance compared to the original paper on the MSCOCO 2014 testing set.

## Implementation
- Data

## Results
<p align="center">
  <img src="https://github.com/xiekt1993/Portfolio/blob/master/Neural_Image_Caption_Generator/examples.png" width="750"/>
</p>

## Reference
Vinyals, Oriol, et al. "Show and tell: A neural image caption generator." Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.
Chen, Xinlei, et al. "Microsoft COCO captions: Data collection and evaluation server." arXiv preprint arXiv:1504.00325(2015).
Xu, Kelvin, et al. "Show, attend and tell: Neural image caption generation with visual attention." International conference on machine learning. 2015.
